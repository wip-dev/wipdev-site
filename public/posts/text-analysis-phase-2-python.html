<!DOCTYPE html><html><head><link href="/styles.css" rel="stylesheet" /></head><body><article>title: "Text Analysis Phase 2: Python"date: 2020-07-28T18:50:42+02:00Description: "I go over the second step in my text analysis and NLP journey."Tags: ["Python", "Text Analysis"]Categories: ["Blog"]DisableComments: false<h2>math: true</h2><p>I have been an R purist through my whole learning journey, but now I am picking up Python. Although I did play with a lot of languages, including Python, my goal was never to make something practical with them. But now, I felt it was necessary to switch to Python for real to learn more natural language processing. I don't believe R is lacking in power in that domain, but I know the Python world is richer with educational resources.</p><p>In this post, I will share some of my thoughts on different parts of the learning experience.</p><h1>First resource (not really)</h1><p>Just as I started learning on R with a <a href='https://www.tidytextmining.com/'>book</a>, I am did the same with Python with <a href='http://www.nltk.org/book/'>The NLTK book</a> by Steven Bird. This book quickly caught my interest because: 1) it is written by the developers of the library NLTK to teach how to use it, and 2) the content assumes no prior programming experience and teaches Python, not just the library.</p><p>I finished 2 chapters so far, and I can't help but compare the learning experience of both resources.</p><ul><li><strong>Density:</strong> The R book had much less content per chapter and was more  hands-on. I believe this is kind of a common theme of the "modern" R books.  The NLTK book, on the other hand, was much more detailed. It was written in a  way to allow its use as a textbook for an academic curriculum with no  requirement of prior programming knowledge.</li><li><strong>Programming:</strong> The tidy text book <i>and</i> package are about a  <a href='https://www.tidytextmining.com/tidytext.html'>"tidy"</a> format of text, which  is created with a simple <code>unnest&#95;tokens&#40;&#41;</code> call and allows analysis with the  popular Tidyverse tools. So, there isn't much new stuff to learn beyond  tidying text data. NLTK deals at a lower level with list- and dictionary-like  objects, which means that most of the processing algorithms are written with  stuff from the standard library. (More on that later.)</li></ul><h1>Jupyter Notebooks</h1><p>I chose to code along the book and solve its exercises in Jupyter Notebooks. (I have them hosted on <a href='https://github.com/waseem-medhat/nltk_book'>GitHub</a>). They seemed like a convenient place to learn as they are interactive and, more or less, self-contained so I could print code results (including plots) in place. So, they are more suitable for someone playing around with code. Plus, I wanted to get started right away and didn't want to spend time setting up Vim.</p><p>To be honest, I don't like Jupyter Notebooks very much. Yes, I do praise their convenience, but I wouldn't use them in a real project except for the initial exploratory work. There is also this little gripe I have about people publishing notebooks on GitHub with absolutely no Markdown content or plots. How different is it from a simple <code>.py</code> script now?</p><h1>Data Structures and Algorithms</h1><p>It came as a (pleasant) surprise that I learned many things in Python that are not specific to NLTK or text analysis. I had practice with a lot of the fundamentals, dealing with dictionaries and lists, iteration, functional programming, refactoring for efficiency, and a little <code>matplotlib</code>.</p><p>The part I captured below is a nice demonstration of what I learned in Python coding. The first algorithm I wrote iterates over a list, <i>over another list</i>. This yields a number of iterations equal to the product of the length of the two lists. It took more than 30 minutes before I actually stopped it from running. Then I rewrote it by making another algorithm that uses exactly one dictionary and no nested iterations. It took about 500 milliseconds.</p><p><i>From half an hour to half a second!</i></p><p>Also, I wrote the dictionary-based algorithm in a functional form and a list comprehension form. I love list comprehensions in Python: they make a really elegant way to make a list with iteration.</p><p><img src="/post/text-analysis-phase-2-python_files/cmu.png" alt="algorithm" /></p><h1>Application</h1><p>Exercises in the NLTK book included some relatively practical text analysis questions. One of them was Zipf's law in which I created a plot similar to the one <a href='/post/first-steps-with-text-analysis/'>I previously made with R</a> but much simpler and uglier. I need to up my game with <code>matplotlib</code>.</p><p><img src="/post/text-analysis-phase-2-python_files/zipf.png" alt="zipf" /></p><p>Another one was to see the frequency of name initials in male and female names.</p><p><img src="/post/text-analysis-phase-2-python_files/initials.png" alt="initials" /></p><hr/><p>So far, I really enjoyed working with Python, both as a text analysis tool and a programming language in general. It has a clean syntax and cool stuff like list (and dict) comprehensions. But what I learned so far in terms of NLP is still simple. I will keep studying from the NLTK book, and surely learn more Python in process.</p></article></body></html>